---
title: "20160922_Thompson_MLE_HW-02"
author: "Nick Thompson"
date: "9/16/2016"
output: pdf_document
header-includes:
- \setlength{\parindent}{4em}
- \setlength{\parskip}{0em}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
rm(list=ls())
cat("\014")
library(dplyr)
library(ggplot2)
library(tidyr)
library(stargazer)
setwd("/Users/Nick/Documents/GitHubRepo/729_Reed_MLE_git/Assignments")
#wdata <- read.table("world.csv", sep = ",",header = T)
#save(wdata, file = "wdata.RData")
load("wdata.RData")
options(scipen=999)
```

<center> <h1>Homework 2 - GVPT 729A</h1> </center>

Answer the following questions. Include your code, and report all the results you used to answer the questions.

https://raw.githubusercontent.com/Neilblund/729A/master/data/world.csv

The dataset at the link above contains information on cross national voter turnout (votevap) among the voting eligible population and per-capita GDP (gdppcap08).

1. Estimate the effect of per-capita GDP on voter turnout using OLS.

```{r ols, warning=FALSE, include=TRUE}
ols_01 <- lm(wdata$vote~wdata$gdppcap08); ols_01
stargazer(ols_01, type = "text")
```

* On average a one unit change in per-capita GDP correlates to a 0.000197 unit change in voter turnout.  Statistically significant at p<0.05.

2. Estimate the effect of per-capita GDP on voter turnout using the maximum likelihood estimator.

```{r MLE estimator, warning=FALSE, message=FALSE, include=TRUE}
attach(wdata)
X <- cbind(1,gdppcap08)
y <- votevap

ols.lf<-function(theta,y,X){
  n<-nrow(X)
  k<-ncol(X)
  beta<-theta[1:k]
  sigma2<-theta[k+1]
  e<-y-X%*%beta
  logl<- -.5*n*log(2*pi)-.5*n*log(sigma2)-
    ((t(e)%*%e)/(2*sigma2))
  return(-logl)
}

p<-optim(c(1,1,1),ols.lf,method="BFGS",hessian=T,y=y,X=X)
p

Fisher.p<-solve(p$hessian)
Fisher.p
sqrt(diag(Fisher.p))
```

3. Take random samples of size 40, 30, and 20 from your dataset. Use the same model you used to answer questions 1 and 2, and compare your results.

```{r Random Samples, include=FALSE}
# set scientific notation
options(scipen = 999)
```

```{r Random Sample OLS 40, warning=FALSE, message=FALSE, include=TRUE}
# Attach Data for Random Sample of 40
set.seed(42)
sample.40 <- wdata[sample(1:nrow(wdata), 40,
  	replace=FALSE),]
attach(sample.40)

# Run OLS model for Random Sample of 40.
ols_40 <- lm(sample.40$vote~sample.40$gdppcap08)
stargazer(ols_40, type = "text")

# Run MLE model for Random Sample of 40.
# Name variables
X <- cbind(1,gdppcap08)
y <- votevap

ols.lf<-function(theta,y,X){
  n<-nrow(X)
  k<-ncol(X)
  beta<-theta[1:k]
  sigma2<-theta[k+1]
  e<-y-X%*%beta
  logl<- -.5*n*log(2*pi)-.5*n*log(sigma2)-
    ((t(e)%*%e)/(2*sigma2))
  return(-logl)
}


p.mle.40<-optim(c(1,1,1),ols.lf,method="BFGS",hessian=T,y=y,X=X)
p.mle.40

Fisher.p.mle.40<-solve(p.mle.40$hessian)
Fisher.p.mle.40
sqrt(diag(Fisher.p.mle.40))

```

```{r Random Sample OLS 30, warning=FALSE, message=FALSE, include = TRUE}
set.seed(43)
sample.30 <- wdata[sample(1:nrow(wdata), 30,
  	replace=FALSE),]
attach(sample.30)

ols_30 <- lm(sample.30$vote~sample.30$gdppcap08)
stargazer(ols_30, type = "text")

X <- cbind(1,gdppcap08)
y <- votevap

ols.lf<-function(theta,y,X){
  n<-nrow(X)
  k<-ncol(X)
  beta<-theta[1:k]
  sigma2<-theta[k+1]
  e<-y-X%*%beta
  logl<- -.5*n*log(2*pi)-.5*n*log(sigma2)-
    ((t(e)%*%e)/(2*sigma2))
  return(-logl)
}

p.mle.30<-optim(c(1,1,1),ols.lf,method="BFGS",hessian=T,y=y,X=X)
p.mle.30

Fisher.p.mle.30<-solve(p.mle.30$hessian)
Fisher.p.mle.30
sqrt(diag(Fisher.p.mle.30))

```

```{r Random Sample OLS 20, warning=FALSE, message=FALSE, include = TRUE}
set.seed(44)
sample.20 <- wdata[sample(1:nrow(wdata), 20,
  	replace=FALSE),]
attach(sample.20)

ols_20 <- lm(sample.20$vote~sample.20$gdppcap08)
stargazer(ols_20, type = "text")
X <- cbind(1,gdppcap08)
y <- votevap

ols.lf<-function(theta,y,X){
  n<-nrow(X)
  k<-ncol(X)
  beta<-theta[1:k]
  sigma2<-theta[k+1]
  e<-y-X%*%beta
  logl<- -.5*n*log(2*pi)-.5*n*log(sigma2)-
    ((t(e)%*%e)/(2*sigma2))
  return(-logl)
}

p.mle.20<-optim(c(1,1,1),ols.lf,method="BFGS",hessian=T,y=y,X=X)
p.mle.20

Fisher.p.mle.20<-solve(p.mle.20$hessian)
Fisher.p.mle.20
sqrt(diag(Fisher.p.mle.20))

```
![](Table1_2.png)


* What differences do you notice between the OLS and MLE results? 

Table 1 shows a side by side comparison of all the results.  The standard errors for the variable `gdppcap08` (OLS) vary slightly (within 0.0002).  The standard errors for the constant (OLS) vary more, but remain within 0.25 of one another.  One the other hand, the standard errors for the MLE are inconsistent.  In this case they reduce, but not in a uniform way.
    


* What characteristics of MLE and OLS estimators explain these differences?

Asymptotic consistency is responsible for the variation in standard errors.

    
## notes

* Turning off scientific notation can make it a little easier to make comparisons between your results. Use the command:

To turn off scientific notation. Turn it back on by resetting ”scipen=0”

* You can format your tables however you see fit, but make sure you include the relevant information.

* http://www.statmethods.net/management/subset.html has code for taking random subsets from a data frame.

* If you want to reproduce your results later, you can set R’s random number seed with this command:

*<span style="color:purple">#the number itself doesn't matter</span>*

```{r set seed, include=TRUE}
set.seed(1000)
```

