---
title: "Hamner & Kalkan Replication"
author: "Nick Thompson"
date: "10/12/2016"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
rm(list=ls())
cat("\014")
# devtools::install_github("chrismeserole/obsval")
library(foreign)
library(stargazer)
library(devtools)
library(obsval)
library(mvtnorm)
library(arm)
library(vioplot)
library(lmtest)
library(knitr)
library(readstata13)
library(ggplot2)
library(moments)
library(MASS)
```

Pull in Data

```{r pull data, echo=TRUE, eval=TRUE}
setwd("~/Documents/GitHubRepo/729_Reed_MLE_git/Exam/hanmer")
#data <- read.csv(file = "https://raw.githubusercontent.com/Neilblund/729A/master/data/voterid.csv", header = TRUE, sep = ",")
#or
#data <- read.dta(file = "hk1.dta")
#or
# data <- read.dta13(file = "hyperlink or filename goes here")
#save(data, file = "data.RData")
load("data.RData")
#View(data)
```

# Introduction:

```{r loda data & write essential variables, echo=TRUE, eval=TRUE}
#descriptive statistics for all variables
#stargazer(voterid, type = 'text')
# run probit, show results
data <- na.omit(data)
#View(data)
#View(data) - here we put the point prediction that we're looking for.  
# If you're using mean, then keep mean.  
# If you're using a SD up and down from the mean, then use that.  
# If you're using a different range, then use that.
data$mean_gopleg <- mean(data$gopleg)
data$sd_gopleg <- 0.5*sd(data$gopleg)
data$med_g = median(data$gopleg)
```

Descriptive statistics 

```{r descriptive statistics stargazer, echo=TRUE, eval=TRUE, results='asis'}
stargazer(data, header = F)
# type = 'text',
```

Plot a histogram to see what the data looks like.  Identify skewness for determining if using mean, median, or tail.

```{r probit plot, echo=FALSE, eval=FALSE}
g <- ggplot(data,aes(x=gopleg))
# adjust the binwidth to the desired width so the histogram tells you something.
# the geom_vlines give you lines to mark the mean and median.
# Consider using the median if the skewness > 0.5 or skewness < -0.5, but use your 
# own discretion.  If the -0.5 < skewness < 0.5 then use the mean. 
# skewness(data$gopleg)
# kurtosis(data$gopleg) # if you care about kurtosis
# anova(model_1p) # if for some reason you want ANOVA
g + geom_histogram(aes(y=..density..),binwidth = 7.5) +
  geom_density() +
  geom_vline(xintercept = data$mean_gopleg,linetype='longdash') +
  geom_vline(xintercept = data$med_g)
gopleg_obs_low <- data$mean_gopleg - data$sd_gopleg
gopleg_obs_high <- data$mean_gopleg + data$sd_gopleg
```

Calculate the average effect of `variable name` using observed values.

The logit model.

```{r logit model - logit link, echo=FALSE, eval=FALSE}
# run probit, show results
(model_1l <- glm('photo ~ fraud + election_margin + gopleg', 
               family = binomial(link = "logit"), 
               data = data)) 
```

A table of the logit model.

```{r logit model stargazer - 2, echo=FALSE, eval=FALSE, results='asis'}
#summary(model_1l)
stargazer(model_1l,header=F) # if you want to see a print out in your console, 
                             # then after model_1l typt this ,type='text',
```

The logit predicted probabilities.

```{r logit predicted probabilities - automatic, echo=FALSE, eval=FALSE}
# generate predicted probabilities automatically
data$pprob_l <- predict(model_1l,type="response")

# generate predicted probabilities manually
data$pprob_manual_l <- pnorm(model_1l$coef['(Intercept)'] +
                                 model_1l$coef['fraud']*data$fraud +
                                 model_1l$coef['election_margin']*data$election_margin +
                                 model_1l$coef['gopleg']*data$gopleg)

# test that we did it right
data$pprob_test_l <- data$pprob_l - data$pprob_manual_l
```

```{r logit test summary, echo=FALSE, eval=FALSE, results='asis'}
summary(data$pprob_test_l) # should be zeros
```

Calculate the average effects with a logit model.

```{r Calculate the average effect with logit, echo=FALSE, eval=FALSE}
#######1. calculate average effect of photo using observed values----
data$pprob_gopleg_upsd_l <- pnorm(model_1l$coef['(Intercept)'] + 
                                 model_1l$coef['fraud']*data$fraud + 
                                 model_1l$coef['election_margin']*data$election_margin + 
                                 model_1l$coef['gopleg']*gopleg_obs_high)
summary(data$pprob_gopleg_upsd_l)

data$pprob_gopleg_downsd_l <- pnorm(model_1l$coef['(Intercept)'] + 
                                 model_1l$coef['fraud']*data$fraud + 
                                 model_1l$coef['election_margin']*data$election_margin + 
                                 model_1l$coef['gopleg']*gopleg_obs_low)
summary(data$pprob_gopleg_downsd_l)

data$pprob_effect_l <- data$pprob_gopleg_upsd_l - data$pprob_gopleg_downsd_l
summary(data$pprob_effect_l)	
```

A way to do this with probit.

```{r probit model - probit link, echo=TRUE, eval=TRUE}
# run probit, show results
(model_1p <- glm('bushvote ~ retecon + partyid +
                 bushiraq + ideol + 
                 white + female + age + 
                 educ1_7 + income', 
               family = binomial(link = "probit"),
               data = data)) 
```

A table printout of the probit model.

```{r probit model stargazer - 2, echo=TRUE, eval=TRUE, results='asis'}
# summary(model_1p)
stargazer(model_1p, title = "H&K Probit Table 1 Replication")
# type = 'text',
# header=F, 
```

Calculate the predicted probabilities in the probit model.

```{r probit predicted probabilities - automatic, echo=FALSE, eval=FALSE, eval=FALSE}
# generate predicted probabilities automatically
data$pprob_p <- predict(model_1p,type="response")

# generate predicted probabilities manually
data$pprob_manual_p <- pnorm(model_1p$coef['(Intercept)'] +
                                 model_1p$coef['fraud']*data$fraud +
                                 model_1p$coef['election_margin']*data$election_margin +
                                 model_1p$coef['gopleg']*data$gopleg)

# test that we did it right
data$pprob_test_p <- data$pprob_p - data$pprob_manual_p
```

A summary of the probit test.

```{r probit test summary, echo=FALSE, eval=FALSE, results='asis'}
summary(data$pprob_test_p) # should be zeros
```

Calculate the average effects and difference for a probit model.

```{r calculate the average effects and difference, echo=FALSE, eval=FALSE}
#######1. calculate average effect of photo using observed values----
data$pprob_gopleg_upsd_p <- pnorm(model_1p$coef['(Intercept)'] + 
                                 model_1p$coef['fraud']*data$fraud + 
                                 model_1p$coef['election_margin']*data$election_margin + 
                                 model_1p$coef['gopleg']*gopleg_obs_high)
summary(data$pprob_gopleg_upsd_p)

data$pprob_gopleg_downsd_p <- pnorm(model_1p$coef['(Intercept)'] + 
                                 model_1p$coef['fraud']*data$fraud + 
                                 model_1p$coef['election_margin']*data$election_margin + 
                                 model_1p$coef['gopleg']*gopleg_obs_low)
summary(data$pprob_gopleg_downsd_p)

data$pprob_effect_p <- data$pprob_gopleg_upsd_p - data$pprob_gopleg_downsd_p
	
```

```{r probit observed difference summary, echo=FALSE, eval=FALSE}
summary(data$pprob_effect_p)
```

Summary:  Interpreting the Coefficients, include the AIC

```{r note to self, echo=FALSE, eval=FALSE, results='asis'}
stargazer(model_1p, header = F)
```

Log Likelihood

```{r loglikelihood - LL, echo=FALSE, eval=FALSE}
# estimate simple logit model
m <- glm(photo ~ fraud + election_margin + gopleg, 
               family = binomial,
               data = data) 
m0<-glm(photo~1, family=binomial, data = data)
#Likelihood Ratio Tests
logLik(m0)
logLik(m)
lr.test = -2*(logLik(m0) - logLik(m))
lr.test
lrtest(m0,m)
```

Deviances
```{r Deviances - D, echo=FALSE, eval=FALSE}
nd<-2*(0 - logLik(m0))
nd
rd<-2*(0-logLik(m))
rd
nd-rd
```

Bayes
```{r bayes - B, echo=FALSE, eval=FALSE}
#Bayesian Information Criterion
BIC<- -2*logLik(m)+log(data$photo)*(length(coef(m))+1)
BIC
```

AIC
```{r AIC - A, echo=FALSE, eval=FALSE}
#Akaikeâ€™s Information Criterion
length(coef(m))
AIC<- -2*(logLik(m))+2*(length(coef(m)))
AIC
```

Simulations:
```{r set the medians - S, echo=FALSE, eval=FALSE}
# set the medians
x.frau <- median(data$fraud)  # in years
x.ele <- median(data$election_margin)  # in years
x.gopleg <- median(data$gopleg)  # in 10,000s of dollars
```

```{r set beta.hat, sigma.hat, and X.c -S, echo=FALSE, eval=FALSE}
# set beta.hat, Sigma.hat, and X.c
beta.hat <- coef(m)
Sigma.hat <- vcov(m)
X.c <- c(1, x.frau, x.ele, x.gopleg)
```

```{r simulate p.tilde - S, echo=FALSE, eval=FALSE}
# simulate p.tilde
n.sims <- 1000  # set number of simulations
p.tilde <- numeric(n.sims)  # create holder for simulations
for (i in 1:n.sims) {
  beta.tilde <- mvrnorm(1, beta.hat, Sigma.hat)
  p.tilde[i] <- plogis(X.c%*%beta.tilde) 
}
```

```{r characterize the simulations - S, echo=FALSE, eval=FALSE}
# characterize the simulations
mean(p.tilde)
sd(p.tilde)
quantile(p.tilde, c(0.05, 0.95))

mod <- obsval('photo~fraud+election_margin+gopleg',data=data,
              reg.model = "logit",
              n.draws = 1000,
              effect.var = "gopleg",
              effect.vals = c(0,2), # lowest to mid
              verbose = TRUE)
```

```{r display results - S, echo=FALSE, eval=FALSE}
# display model results
summary(mod$model)
summary(mod$preds)
# see the names of everything obsval returns
# names(mod)
```

Violin Plots of the Simulation
```{r violin plot of the simulation - V, echo=FALSE, eval=FALSE}
vioplot(mod$sim.coef[,1], mod$sim.coef[,2], mod$sim.coef[,3], 
        mod$sim.coefs[,4], names=c(expression(beta[0]),expression(beta[fraud]),
                        expression(beta[election_margin]),expression(beta[gopleg])), 
        col="red")
title("Coefficients")
```

```{r violin plot, echo=FALSE, eval=FALSE}
vioplot(mod$preds[,1], mod$preds[,2], names=c("age=0","age=2"), 
        col="red")
title("Predicted Probability")
mean(mod$preds[, 1] - mod$preds[, 2])
# calculate difference between quantiles
quantile(mod$preds[, 1] - mod$preds[, 2], c(0.025, 0.975))
```

