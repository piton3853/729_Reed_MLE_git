---
title: "Exam Practic 2"
author: "Nick Thompson"
date: "10/13/2016"
output: pdf_document
---

```{r setup, echo=T, eval=T, include=F, cache=F, results='asis'}
knitr::opts_chunk$set(echo = TRUE)
rm(list=ls())
cat("\014")
# devtools::install_github("chrismeserole/obsval")
library(devtools)
library(mvtnorm)
library(arm)
library(lmtest)
library(knitr)
library(readstata13)
library(ggplot2)
library(moments)
library(MASS)
library(foreign)
library(obsval)
library(stargazer)
library(vioplot)
library(lmtest)
```

Pull in Data

```{r, set working dir and load data, echo=T, eval=T, include=T, cache=F, results='asis'}
setwd("~/Documents/GitHubRepo/729_Reed_MLE_git/Exam/CCES")
#data <- read.csv(file = "https://raw.githubusercontent.com/Neilblund/729A/master/data/voterid.csv", header = TRUE, sep = ",")
#or
# data <- read.dta(file = "CCES.RData")
#or
# data <- read.dta13(file = "CCES.RData")
# save(data, file = "data.RData")
load("CCES.RData")
data <- x
#View(data)
```

# Introduction:

```{r loda data & write essential variables, echo=T, eval=T, include=T, cache=F, results='asis'}
#descriptive statistics for all variables
#stargazer(voterid, type = 'text')
# run probit, show results
data <- na.omit(data)
#View(data) - here we put the point prediction that we're looking for.  
# If you're using mean, then keep mean.  
# If you're using a SD up and down from the mean, then use that.  
# If you're using a different range, then use that.
data$var1 = data$age
data$var2 = data$income
data$var3 = data$race
data$var4 = data$divergence
data$var5 = data$educ
data$var6 = data$female
data$var7 = data$competitive
data$var8 = data$st_id
data$var9 <- data$voted
data$mean_var1 <- mean(data$var1, na.rm=T)
data$sd_var1 <- 0.5*sd(data$var1)
data$med_var1 = median(data$var1)
```

Descriptive statistics 

```{r descriptive statistics stargazer, echo=F, eval=F, include=F, results='asis'}
stargazer(data, type = 'text', header = F)
```

Plot a histogram to see what the data looks like.  Identify skewness for determining if using mean, median, or tail.

```{r independent variable plot, echo=T, eval=T, include=T, cache=F, results='asis'}
g <- ggplot(data,aes(x=var1))
# adjust the binwidth to the desired width so the histogram tells you something.
# the geom_vlines give you lines to mark the mean and median.
# Consider using the median if the skewness > 0.5 or skewness < -0.5, but use your 
# own discretion.  If the -0.5 < skewness < 0.5 then use the mean. 
skewness(data$var1)
# kurtosis(data$var1) # if you care about kurtosis
# anova(model_1p) # if for some reason you want ANOVA
g + geom_histogram(aes(y=..density..),binwidth = .15) +
  geom_density() +
  geom_vline(xintercept = data$mean_var1,linetype='longdash') +
  geom_vline(xintercept = data$med_var1)
data$var1_obs_low <- data$mean_var1 - data$sd_var1
data$var1_obs_high <- data$mean_var1 + data$sd_var1
```

Calculate the average effect of `variable name` using observed values.

Neil's `simple_analysis.R` code for predicted values (my results should be different from the author's).

```{r simple_analysis, echo=T, eval=T, include=T, cache=F, results='asis'}
#using obsval for logistic regression and prediction----

# nick's model
# run binomial, show results
model_1b <-obsval(voted~age+income+
                 as.factor(race)+divergence+educ+
                 female+competitive+as.factor(st_id),
               data=data,
               reg.model = "logit",
               n.draws = 1000,
               "effect.var"="divergence",
               effect.vals=c(0,1),
               verbose=T,
               baseline.category="Voted")

mymodel<-obsval(binary_warsup~female, #the regression model y~x1+x2+x3
                  data=studentVote,         #the dataset
                  reg.model="logit",          #the link function
                  n.draws=1000,             #the number of random draws
                   "effect.var"="female",       #the variable of interest
                  effect.vals=c(0,1) ,       #the low and high values of the IV of interest
                   verbose=TRUE, 
                  baseline.category=oppose) #this is the baseline category of your dependent variable

#results----

#this gets the basic model output
summary(mymodel$model)

#average effect of moving from low to high
mymodel$effect.mean
#low ci
mymodel$effect.low.ci
#high ci
mymodel$effect.high.ci

#predicted probability
mymodel$means

#plots----

#violin plots
plot.new()
vioplot(mymodel$preds[,1], mymodel$preds[,2], names=c("male","female"), 
        col="red", horizontal=TRUE)
title("Predicted Probability")

#coefficient plots
vioplot(mymodel$sim.coef[,1], mymodel$sim.coef[,2], names=c(expression(beta[0]),expression(beta[age])))

#histogram of predicted effect sizes
hist(mymodel$preds[,1]-mymodel$preds[,2], main="Simulated Effects", xlab="effects")


#Model fit----
AIC(mymodel$model)
BIC(mymodel$model)

#comparison with null model
nullModel<-glm(studentVote$binary_warsup~1, family="binomial")

#likelihood ratio test (using base R)
lr.test = -2*(logLik(nullModel) - logLik(mymodel$model))
1-pchisq(as.numeric(lr.test), df=1)


#likelihood ratio test (using lmtest)
library(lmtest)
lrtest(mymodel$model, nullModel)
```



The binomial model.

```{r binomial model, echo=T, eval=T, include=T, cache=F, results='asis'}
# run binomial, show results
model_1b <-glm(voted~age+income+
                 as.factor(race)+divergence+educ+
                 female+competitive+as.factor(st_id),
               family="binomial",data=data)
```

A table of the logit model.

```{r binomial model stargazer, echo=F, eval=F, include=F, cache=F, results='asis'}
summary(model_1l)
stargazer(model_1l,header=F) # if you want to see a print out in your console, 
                             # then after model_1l typt this 
# ,type='text',
```

The logit predicted probabilities.

```{r logit predicted probabilities, echo=T, eval=T, include=T, cache=F, results='asis'}
# generate predicted probabilities automatically
data$pprob_l <- predict(model_1l,type="response")

# generate predicted probabilities manually
data$pprob_manual_l <- pnorm(model_1l$coef['(Intercept)'] +
                               model_1l$coef['var1']*data$var1 +
                               model_1l$coef['var2']*data$var2 +
                               model_1l$coef['var3']*data$var3 +
                               model_1l$coef['var4']*data$var4 +
                               model_1l$coef['var5']*data$var5 +
                               model_1l$coef['var6']*data$var6 +
                               model_1l$coef['var7']*data$var7 +
                               model_1l$coef['var8']*data$var8)
  

# test that we did it right
data$pprob_test_l <- data$pprob_l - data$pprob_manual_l
```

```{r logit test summary, echo=F, eval=F, include=F, cache=F, results='asis'}
summary(data$pprob_test_l) # should be zeros
```

Calculate the average effects with a logit model.

```{r Calculate the average effect with logit, echo=F, eval=F, include=F, cache=F, results='asis'}
#######1. calculate average effect of photo using observed values----
data$pprob_var1_upsd_l <- pnorm(model_1l$coef['(Intercept)'] + 
                                 model_1l$coef['fraud']*data$fraud + 
                                 model_1l$coef['election_margin']*data$election_margin + 
                                 model_1l$coef['var1']*var1_obs_high)
summary(data$pprob_var1_upsd_l)

data$pprob_var1_downsd_l <- pnorm(model_1l$coef['(Intercept)'] + 
                                 model_1l$coef['fraud']*data$fraud + 
                                 model_1l$coef['election_margin']*data$election_margin + 
                                 model_1l$coef['var1']*var1_obs_low)
summary(data$pprob_var1_downsd_l)

data$pprob_effect_l <- data$pprob_var1_upsd_l - data$pprob_var1_downsd_l
summary(data$pprob_effect_l)	
```

A way to do this with probit.

```{r probit model - probit link, echo=F, eval=F, include=F, cache=F, results='asis'}
# run probit, show results
(model_1p <- glm('photo ~ fraud + election_margin + var1', 
               family = binomial(link = "probit"), 
               data = data)) 
```

A table printout of the probit model.

```{r probit model stargazer - 2, echo=F, eval=F, include=F, cache=F, results='asis'}
#summary(model_1p)
stargazer(model_1p,type = 'text',header=F)
```

Calculate the predicted probabilities in the probit model.

```{r probit predicted probabilities - automatic, echo=F, eval=F, include=F, cache=F, results='asis'}
# generate predicted probabilities automatically
data$pprob_p <- predict(model_1p,type="response")

# generate predicted probabilities manually
data$pprob_manual_p <- pnorm(model_1p$coef['(Intercept)'] +
                                 model_1p$coef['fraud']*data$fraud +
                                 model_1p$coef['election_margin']*data$election_margin +
                                 model_1p$coef['var1']*data$var1)

# test that we did it right
data$pprob_test_p <- data$pprob_p - data$pprob_manual_p
```

A summary of the probit test.

```{r probit test summary, echo=F, eval=F, include=F, cache=F, results='asis'}
summary(data$pprob_test_p) # should be zeros
```

Calculate the average effects and difference for a probit model.

```{r calculate the average effects and difference, echo=F, eval=F, include=F, cache=F, results='asis'}
#######1. calculate average effect of photo using observed values----
data$pprob_var1_upsd_p <- pnorm(model_1p$coef['(Intercept)'] + 
                                 model_1p$coef['fraud']*data$fraud + 
                                 model_1p$coef['election_margin']*data$election_margin + 
                                 model_1p$coef['var1']*var1_obs_high)
summary(data$pprob_var1_upsd_p)

data$pprob_var1_downsd_p <- pnorm(model_1p$coef['(Intercept)'] + 
                                 model_1p$coef['fraud']*data$fraud + 
                                 model_1p$coef['election_margin']*data$election_margin + 
                                 model_1p$coef['var1']*var1_obs_low)
summary(data$pprob_var1_downsd_p)

data$pprob_effect_p <- data$pprob_var1_upsd_p - data$pprob_var1_downsd_p
	
```

```{r probit observed difference summary, echo=F, eval=F, include=F, cache=F, results='asis'}
summary(data$pprob_effect_p)
```

Summary:  Interpreting the Coefficients, include the AIC

```{r note to self, echo=F, eval=F, include=F, cache=F, results='asis'}
stargazer(model_1p, header = F)
```

Log Likelihood

```{r loglikelihood - LL, echo=F, eval=F, include=F, cache=F, results='asis'}
# estimate simple logit model
m <- glm(photo ~ fraud + election_margin + var1, 
               family = binomial,
               data = data) 
m0<-glm(photo~1, family=binomial, data = data)
#Likelihood Ratio Tests
logLik(m0)
logLik(m)
lr.test = -2*(logLik(m0) - logLik(m))
lr.test
lrtest(m0,m)
```

Deviances
```{r Deviances - D, echo=F, eval=F, include=F, cache=F, results='asis'}
nd<-2*(0 - logLik(m0))
nd
rd<-2*(0-logLik(m))
rd
nd-rd
```

Bayes
```{r bayes - B, echo=F, eval=F, include=F, cache=F, results='asis'}
#Bayesian Information Criterion
BIC<- -2*logLik(m)+log(data$photo)*(length(coef(m))+1)
BIC
```

AIC
```{r AIC - A, echo=F, eval=F, include=F, cache=F, results='asis'}
#Akaikeâ€™s Information Criterion
length(coef(m))
AIC<- -2*(logLik(m))+2*(length(coef(m)))
AIC
```

Simulations:
```{r set the medians - S, echo=F, eval=F, include=F, cache=F, results='asis'}
# set the medians
x.frau <- median(data$fraud)  # in years
x.ele <- median(data$election_margin)  # in years
x.var1 <- median(data$var1)  # in 10,000s of dollars
```

```{r set beta.hat, sigma.hat, and X.c -S, echo=F, eval=F, include=F, cache=F, results='asis'}
# set beta.hat, Sigma.hat, and X.c
beta.hat <- coef(m)
Sigma.hat <- vcov(m)
X.c <- c(1, x.frau, x.ele, x.var1)
```

```{r simulate p.tilde - S, echo=F, eval=F, include=F, cache=F, results='asis'}
# simulate p.tilde
n.sims <- 1000  # set number of simulations
p.tilde <- numeric(n.sims)  # create holder for simulations
for (i in 1:n.sims) {
  beta.tilde <- mvrnorm(1, beta.hat, Sigma.hat)
  p.tilde[i] <- plogis(X.c%*%beta.tilde) 
}
```

```{r characterize the simulations - S, echo=F, eval=F, include=F, cache=F, results='asis'}
# characterize the simulations
mean(p.tilde)
sd(p.tilde)
quantile(p.tilde, c(0.05, 0.95))

data$mod <- obsval('photo~fraud+election_margin+var1',data=data,
              reg.model = "logit",
              n.draws = 1000,
              effect.var = "var1",
              effect.vals = c(0,2), # lowest to mid
              verbose = TRUE)
```

```{r display results - S, echo=F, eval=F, include=F, cache=F, results='asis'}
# display model results
summary(mod$model)
summary(mod$preds)
# see the names of everything obsval returns
# names(mod)
```

Violin Plots of the Simulation
```{r violin plot of the simulation - V, echo=F, eval=F, include=F, cache=F, results='asis'}
vioplot(mod$sim.coef[,1], mod$sim.coef[,2], mod$sim.coef[,3], 
        mod$sim.coefs[,4], names=c(expression(beta[0]),expression(beta[fraud]),
                        expression(beta[election_margin]),expression(beta[var1])), 
        col="red")
title("Coefficients")
```

```{r violin plot, echo=F, eval=F, include=F, cache=F, results='asis'}
vioplot(mod$preds[,1], mod$preds[,2], names=c("age=0","age=2"), 
        col="red")
title("Predicted Probability")
mean(mod$preds[, 1] - mod$preds[, 2])
# calculate difference between quantiles
quantile(mod$preds[, 1] - mod$preds[, 2], c(0.025, 0.975))
```

